I would like to work on fine-tuning Large language models to perform some downstream natural language processing tasks. One such task is commonsense reasoning. I recently came across a recently released benchmark dataset that is well suited for this type of task (https://arxiv.org/pdf/2210.01241.pdf). A promising approach is to use a reinforcement learning method to train a language model that was pre-trained in a supervised fashion. The above benchmark also provides a reinforcement learning environment for language tasks that can be used to train this type of models. Since the reasoning task boils down to text generation, one common metric used to evaluate the performance is BLEU scores.
